{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import random as ra\n",
    "import numpy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "import sklearn.metrics as metrics\n",
    "from Juliet2_Schema import Cases, VLW, engine\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Connection to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(name):\n",
    "    #filename= './'+name+'_LSTM_DATA.json'\n",
    "    #segments = json.load(open('./LSTM_DATA.json'))\n",
    "    \n",
    "    segments = json.load(open('./'+name+'_LSTM_DATA.json'))\n",
    "\n",
    "    #print data info\n",
    "    print('number of code snippet samples:',len(segments))\n",
    "    #total samples wtihin snippets\n",
    "    count = 0\n",
    "    #find number of samples in a snippet\n",
    "    for s in segments:\n",
    "        count+=len(segments[s]['x'])\n",
    "\n",
    "    print('number of total samples from parsed snippets:',count)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def get_sampled_indexes(segment_length,num,frac=False):\n",
    "    if frac:\n",
    "        sampled_indexes = ra.sample(range(0,segment_length),int(segment_length/num))\n",
    "    else:\n",
    "        sampled_indexes = ra.sample(range(0,segment_length),num)\n",
    "        \n",
    "    return sampled_indexes\n",
    "\n",
    "\n",
    "def get_samples(segments, num_train, num_val, num_test, mult = 1 ):\n",
    "    \n",
    "    #multi serves to increase the number of samples and number sampled from data\n",
    "    \n",
    "    #TODO: Update these labels and change as necessary\n",
    "    train_x= []\n",
    "    train_y=[]\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "\n",
    "    for s in segments:\n",
    "\n",
    "\n",
    "        #only use X amount of data from each segment\n",
    "        #to reduce data size and train on more samples (so more representive of whole data set)\n",
    "        segment_length = len(segments[s]['x'])\n",
    "        sampled_indexes = get_sampled_indexes(segment_length,mult)\n",
    "\n",
    "        for ix,x in enumerate(segments[s]['x']):\n",
    "\n",
    "            y = segments[s]['y'][ix]\n",
    "\n",
    "            #must be of correct shape to be used\n",
    "            if len(x)==41 and len(y)==41 and ix in sampled_indexes: #and 1 in y:\n",
    "                if len(train_x)<num_train*mult:\n",
    "                    train_x.append(x)\n",
    "                    train_y.append(y)\n",
    "                elif len(val_x)<num_val*mult:\n",
    "                    val_x.append(x)\n",
    "                    val_y.append(y)\n",
    "                elif len(test_x)<num_test*mult:\n",
    "                    test_x.append(x)\n",
    "                    test_y.append(y)\n",
    "                else:\n",
    "                    #extra data\n",
    "                    pass\n",
    "                \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "#'''\n",
    "\n",
    "\n",
    "    \n",
    "def train_lm(model, train_data, val_data, \n",
    "             word_vectors, num_train, num_val, name,\n",
    "             batch_size=50, lr=0.01, mu=0.25):\n",
    "    # setup our optimizer and loss function\n",
    "    opt = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    torch.manual_seed(691)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    epoch = 0\n",
    "    no_imp = 0\n",
    "    patience=5\n",
    "    \n",
    "    # May want to increase the Max epochs!\n",
    "    max_epochs = 50\n",
    "    training = True\n",
    "    best_val_loss = None\n",
    "    \n",
    "    history = {}\n",
    "    \n",
    "    training=True\n",
    "\n",
    "    #while (training and not max_epoch) or (epoch < max_epoch):\n",
    "    #while training and epoch < max_epoch:\n",
    "    while epoch < max_epochs and no_imp < patience:\n",
    "        print(f'Start epoch {epoch + 1}')\n",
    "\n",
    "        # Turn on training mode which enables dropout.\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        vocab_size = word_vectors.shape[0]\n",
    "        #print('----')\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            ###\n",
    "            ###MAKE SOME PRINTOUTS\n",
    "            ###\n",
    "            # clear any remaining gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            y_preds, h = model(to_gpu(batch['x']))\n",
    "            \n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_fn(y_preds.view(-1), to_gpu(batch['y'].view(-1)))\n",
    "            train_loss += loss.cpu().item()\n",
    "            #print(train_loss)\n",
    "            \n",
    "            # Calculate the gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # `clip_grad_norm` applies gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), mu)\n",
    "\n",
    "            # update the parameteres yet after clipping\n",
    "            opt.step()\n",
    "\n",
    "        print(f'Current train loss: {train_loss:.2f}')\n",
    "        print(f'Average per sample train loss: {(train_loss/num_train):.2f}')\n",
    "\n",
    "        # disable gradients during validation\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        for batch in val_loader:\n",
    "            y_preds, h = model(to_gpu(batch['x']))\n",
    "            val_loss += loss_fn(y_preds.view(-1), to_gpu(batch['y'].view(-1))).cpu().item()\n",
    "\n",
    "        print(f'Total validation loss: {val_loss:.2f}')\n",
    "        print(f'Average per sample validation loss: {(val_loss/num_val):.2f}')\n",
    "        \n",
    "        if best_val_loss is None or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_imp = 0\n",
    "            torch.save(model.state_dict(), './data/'+name+'biLSTM_LM.pt')\n",
    "        else:\n",
    "            no_imp += 1\n",
    "\n",
    "        print(f'Best validation loss: {best_val_loss:.2f} (Epochs without improvement: {no_imp})')\n",
    "        \n",
    "        '''\n",
    "        if not best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        elif val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            print('No improvement! Early stopping')\n",
    "            training = False\n",
    "        '''\n",
    "\n",
    "    \n",
    "        epoch += 1\n",
    "        \n",
    "        history['epoch_'+str(epoch)]={'train_loss':train_loss,'val_loss':val_loss}\n",
    "        print()\n",
    "\n",
    "    return model,history\n",
    "\n",
    "\n",
    "###\n",
    "# adapted from the PyTorch examples. for the full PyTorch LM example, see: \n",
    "# https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "###\n",
    "\n",
    "class LSTM_LM(nn.Module):\n",
    "    \"\"\"Model feeds pre-trained embeddings through a series of biLSTM\n",
    "       layers, followed by a linear vocabulary decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, lstm_layers, word_vectors, \n",
    "                 dropout=0.05, bidirectional = True):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "\n",
    "        self.vocab_size = word_vectors.shape[0]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "\n",
    "        # blank embed layer starting from GloVe pre-trained vectors\n",
    "        self._embed = nn.Embedding.from_pretrained(word_vectors, freeze=False)        \n",
    "        self._drop = nn.Dropout(dropout)\n",
    "\n",
    "        self._lstm = nn.LSTM(in_dim, hidden_dim, num_layers = lstm_layers, dropout = dropout,\n",
    "                             bidirectional = bidirectional, batch_first=True)\n",
    "        self._ReLU = nn.ReLU()\n",
    "        self._pred = nn.Linear((2 if bidirectional else 1)*hidden_dim, \n",
    "                               #self.vocab_size)\n",
    "                               1) #only 1 or zeros here \n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self._drop(self._embed(x))\n",
    "        z, h = self._lstm(e)\n",
    "        z_drop = self._drop(z)\n",
    "        s = self._pred(self._ReLU(z_drop))\n",
    "        #s = s.view(-1, self.vocab_size)\n",
    "        s = s.squeeze()\n",
    "        return s, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.lstm_layers, batch_size, self.hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the execute model function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_code_segmentation_model(name):\n",
    "    \n",
    "    #add name param\n",
    "    segments = get_segments(name)\n",
    "\n",
    "    num_train = 5000\n",
    "    num_val   = 1500\n",
    "    num_test  = 1500\n",
    "\n",
    "    #split data\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = get_samples(segments, num_train, num_val, num_test)\n",
    "\n",
    "\n",
    "    print('Data Set Up')\n",
    "    print('number of training samples:',len(train_x))\n",
    "    print('number of validation samples:',len(val_x))\n",
    "    print('number of testing samples:',len(test_x))\n",
    "\n",
    "    LM_train_loader = code_snippet_loader(train_x, train_y)\n",
    "    LM_val_loader = code_snippet_loader(val_x, val_y)\n",
    "    LM_test_loader = code_snippet_loader(test_x, test_y)\n",
    "\n",
    "    # considering what our new input, target pairs look like:\n",
    "    print(len(LM_train_loader.x), LM_train_loader[3])\n",
    "\n",
    "\n",
    "    torch.manual_seed(691)\n",
    "\n",
    "    #vocab size from sentence peice\n",
    "    #vocab dim???? \n",
    "    vocab_size = 10000 #same as sentence peice\n",
    "    vocab_dim = 50  # the size of our pre-trained word vectors\n",
    "\n",
    "    # randomly initialize our word vectors!\n",
    "    vocab_dim = 256\n",
    "    word_vectors = torch.randn(vocab_size, vocab_dim)\n",
    "    word_vectors.shape, word_vectors\n",
    "\n",
    "    #show model\n",
    "    hidden_dim = 200\n",
    "    lstm_layers = 2\n",
    "    #LSTM_LM_net = to_gpu(LSTM_LM(newstweet_wvs.shape[1], hidden_dim,lstm_layers, newstweet_wvs))\n",
    "    LSTM_LM_net = LSTM_LM(word_vectors.shape[1], hidden_dim,lstm_layers, word_vectors)\n",
    "\n",
    "    LSTM_LM_net\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    #batch_size = 50 \n",
    "    batch_size = 50\n",
    "\n",
    "    # an initial learning rate, prior to clipping\n",
    "    lr = 0.0005\n",
    "    #lr = 0.01\n",
    "\n",
    "    # the clipping theshold\n",
    "    mu = 0.25\n",
    "\n",
    "    LSTM_LM_net_trained, history = train_lm(LSTM_LM_net, LM_train_loader, LM_val_loader, \n",
    "                                            word_vectors, num_train, num_val, name,\n",
    "                                            batch_size=batch_size, lr=lr, mu=mu)\n",
    "\n",
    "\n",
    "    #show \n",
    "\n",
    "    training_loss_values = []\n",
    "    validation_loss_values = []\n",
    "    for loss in history:\n",
    "\n",
    "        #normal\n",
    "        #training_loss_values.append(history[loss]['train_loss'])\n",
    "        #validation_loss_values.append(history[loss]['val_loss'])\n",
    "        #\n",
    "        #adjusted normal\n",
    "        training_loss_values.append(history[loss]['train_loss']/num_train)\n",
    "        validation_loss_values.append(history[loss]['val_loss']/num_val)\n",
    "        #\n",
    "        #log\n",
    "        #training_loss_values.append(np.log(history[loss]['train_loss']))\n",
    "        #validation_loss_values.append(np.log(history[loss]['val_loss']))\n",
    "        #\n",
    "        #adjusted log\n",
    "        #training_loss_values.append(np.log(history[loss]['train_loss']/num_train))\n",
    "        #validation_loss_values.append(np.log(history[loss]['val_loss']/num_val))\n",
    "\n",
    "\n",
    "    Epochs = range(len(training_loss_values))\n",
    "\n",
    "\n",
    "    #show all\n",
    "    plt.plot(Epochs, training_loss_values)\n",
    "    plt.plot(Epochs, validation_loss_values)\n",
    "    #show all but the first one\n",
    "    #plt.plot(Epochs[1:], training_loss_values[1:])\n",
    "    #plt.plot(Epochs[1:], validation_loss_values[1:])\n",
    "    plt.title('Average loss per sample')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(LM_test_loader, batch_size=num_test, shuffle=True)\n",
    "    \n",
    "    \n",
    "    #SHOW RESUTLS on all Tokens\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    for batch in test_loader:\n",
    "\n",
    "        #get predicted values\n",
    "        y_preds, h = LSTM_LM_net_trained(batch['x'])\n",
    "        y_preds = torch.flatten(torch.sigmoid(y_preds))\n",
    "        y_preds= y_preds.detach().numpy()\n",
    "\n",
    "        #get true values\n",
    "        y = batch['y'].view(-1)\n",
    "        y = y.detach().numpy()\n",
    "\n",
    "        #accuracy\n",
    "        acc = metrics.accuracy_score(y, numpy.rint(y_preds))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y, y_preds)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "    print('Accuracy:',acc)\n",
    "    print('Area under curve:',roc_auc)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) on Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    #instantiate tokenizer model\n",
    "    \n",
    "    class Tokenizer:\n",
    "    \n",
    "        def __init__(self, filepath=name+'_tokenizer.model'):\n",
    "            self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "        def encode(self, text, t=int):\n",
    "            return self.sp.encode(text, out_type=t)\n",
    "\n",
    "        def decode(self, pieces):\n",
    "            return self.sp.decode(pieces)\n",
    "\n",
    "        @staticmethod\n",
    "        def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "            spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                           #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                           input_sentence_size=number_of_lines, shuffle_input_sentence=True)\n",
    "\n",
    "    \n",
    "    tokenizer = Tokenizer(name+'_tokenizer.model')\n",
    "    \n",
    "    \n",
    "    #make a _NEW_LINE_ ROC curve since thats what we care about\n",
    "    test_loader = torch.utils.data.DataLoader(LM_test_loader, batch_size=num_test, shuffle=True)\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    for batch in test_loader:\n",
    "\n",
    "        #get predicted values\n",
    "        y_preds, h = LSTM_LM_net_trained(batch['x'])\n",
    "        y_preds = torch.flatten(torch.sigmoid(y_preds))\n",
    "        y_preds= y_preds.detach().numpy()\n",
    "\n",
    "        #get true values\n",
    "        y = batch['y'].view(-1)\n",
    "        y = y.detach().numpy()\n",
    "\n",
    "        #filter for only new line places (get info from batch['x'])\n",
    "        new_y=[]\n",
    "        new_y_preds=[]\n",
    "        tokens = torch.flatten(batch['x']).detach().numpy()\n",
    "        for i in range(len(tokens)):\n",
    "            tok_literal=tokenizer.decode(int(tokens[i]))\n",
    "            if tok_literal[-7:]=='NEWLINE':\n",
    "                #only rate actual break places\n",
    "                #if y[i]==1:\n",
    "                #    new_y.append(y[i])\n",
    "                #    new_y_preds.append(y_preds[i])\n",
    "                new_y.append(y[i])\n",
    "                new_y_preds.append(y_preds[i])\n",
    "        #accuracy\n",
    "        #https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
    "        acc = metrics.accuracy_score(new_y, numpy.rint(new_y_preds))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, threshold = metrics.roc_curve(new_y, new_y_preds)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    print('Accuracy:',acc)\n",
    "    print('Area under curve:',roc_auc)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) on Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "        \n",
    "#train_code_segmentation_model('py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
