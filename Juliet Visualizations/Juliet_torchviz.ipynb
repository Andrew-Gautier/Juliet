{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded successfully.\n",
      "torch.Size([49152, 4096])\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(49152, 4096)\n",
      "  (rnn): LSTM(4096, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=40, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchviz\n",
    "from torchviz import make_dot\n",
    "from torch.optim import Adam\n",
    "import graphviz\n",
    "import tqdm\n",
    "import os\n",
    "import numpy\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "LSTM_NODES = 256\n",
    "NUM_SENTENCES = 40\n",
    "SENTENCE_LENGTH = 98\n",
    "VOCAB_SIZE = 49152\n",
    "EMBEDDING_SIZE = 4096\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(691)\n",
    "\n",
    "# TODO: Model checkpoints, Model saving\n",
    "try:\n",
    "    pretrained_weights = torch.load(r'C:\\Users\\Andrew\\Desktop\\nasa_project\\Juliet\\Large Files\\aix3-7b-base (1).pt')\n",
    "    print(\"Weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load weights: {e}\")\n",
    "\n",
    "# Once you've identified the key for the embeddings, you can extract them like this:\n",
    "word_vectors = pretrained_weights['tok_embeddings.weight']\n",
    "print(word_vectors.shape)\n",
    "\n",
    "\n",
    "dummy_input = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SENTENCE_LENGTH, NUM_SENTENCES))\n",
    "\n",
    "class LSTMClassifier(nn.Module):    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, batch_first, bidirectional, dropout, pretrained_weights, batch_size, sentence_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding.weight.data.copy_(pretrained_weights)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        batch_size, sentence_length, num_sentences = text.size()\n",
    "        text = text.view(batch_size, sentence_length * num_sentences)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        #print(f\"Shape after embedding: {embedded.shape}\")\n",
    "        lstm_output, (hidden, _) = self.rnn(embedded)\n",
    "        # print(f\"Shape after LSTM: {lstm_output.shape}\")\n",
    "        pooled = torch.mean(lstm_output, dim=1)\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(self.dropout(pooled))\n",
    "       \n",
    "        output = torch.sigmoid(output)\n",
    "        # print(f\"Shape of final output: {output.shape}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "Juliet_Max_Pool = LSTMClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_SIZE,  \n",
    "    hidden_dim=LSTM_NODES,\n",
    "    output_dim=40,  \n",
    "    n_layers=2,\n",
    "    batch_first=True,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    "    pretrained_weights=word_vectors,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sentence_length=SENTENCE_LENGTH\n",
    ")\n",
    "print(Juliet_Max_Pool)\n",
    "\n",
    "model_output = Juliet_Max_Pool(dummy_input)\n",
    "\n",
    "# Visualize the model\n",
    "# dot = make_dot(model_output, params=dict(Juliet_Max_Pool.named_parameters()))\n",
    "\n",
    "\n",
    "# To render and view the graph (requires Graphviz installed in your system)\n",
    "#dot.render('max_pool_model_visualization', format='png', cleanup=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(dot.exe:17008): Pango-WARNING **: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Juliet_Max_Pool.gv.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "\n",
    "\n",
    "\n",
    "model_graph = draw_graph(\n",
    "    model=Juliet_Max_Pool,  \n",
    "    input_data=dummy_input,  \n",
    "    graph_dir='LR',\n",
    "    depth=3,\n",
    "    graph_name='Juliet_Max_Pool',\n",
    "    hide_module_functions=True,\n",
    "    roll=True\n",
    ")\n",
    "\n",
    "\n",
    "model_graph.visual_graph\n",
    "model_graph\n",
    "model_graph.resize_graph(scale=5.0) # scale as per the view model_graph.visual_graph.render(format='svg') \n",
    "model_graph.visual_graph.render(format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
